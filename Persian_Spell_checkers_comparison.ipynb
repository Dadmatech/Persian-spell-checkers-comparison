{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxvDbbA44yuA"
      },
      "source": [
        "# ارزیابی اصلاح گر املایی فارسی"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSOKupJG-DZr"
      },
      "source": [
        "# کد"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7X3uitE-HLc"
      },
      "source": [
        "## آماده‌سازی"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[دانلود دادگان](https://drive.google.com/drive/folders/1JhU8JQI5bLiLbWdvDJqvDTJpRm_D-Xa0?usp=sharing)"
      ],
      "metadata": {
        "id": "HX1Fk_09wIW5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkS7Vab8UP3t",
        "outputId": "c28f5084-ef41-4d40-8883-e6029a7d82dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Dadmatech/spell_checker\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/Dadmatech/spell_checker/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keZ33ZFxUfzz"
      },
      "outputs": [],
      "source": [
        "with open('./data/chars.txt') as f:\n",
        "  chars = f.readlines()\n",
        "char_dict = {c.split(', ')[0]:c.split(', ')[1][0] for c in chars}\n",
        "\n",
        "def char_refinement(text):\n",
        "  s = ''\n",
        "  for i in text:\n",
        "    if i in normal_chars:\n",
        "      s += normal_chars[i]\n",
        "    elif i in ['\\u200c', '_', ' ']:\n",
        "      s += i\n",
        "    elif not i in special_chars:\n",
        "      s += i\n",
        "  return s\n",
        "# char_refinement('یعنی توی برف و زمستان البته برای روستا تعدادی کانکس آهنی آورده بودند')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogg-Nzi0_eMH"
      },
      "outputs": [],
      "source": [
        "normal_chars = {}\n",
        "for i in char_dict:\n",
        "  if char_dict[i] != '\\n':\n",
        "    normal_chars[i] = char_dict[i]\n",
        "# normal_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQmLYn_fJ5kG"
      },
      "outputs": [],
      "source": [
        "special_chars = []\n",
        "for i in char_dict:\n",
        "  if char_dict[i] == '\\n':\n",
        "    special_chars.append(i)\n",
        "# special_chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBpQ-DYqMtu"
      },
      "source": [
        "## خواندن فایل‌ها"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuBVPc_SnCoX"
      },
      "outputs": [],
      "source": [
        "def read_dataset (dataset_name, dataset_type):\n",
        "  with open(f'data/{dataset_name}/{dataset_name}_{dataset_type}.txt') as f:\n",
        "    data = f.readlines()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbw0_zQgnD-W",
        "outputId": "05ceb7cc-7ab2-472d-fea7-36c8430def08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lengths: 1033, 1033, 1033, 1033\n"
          ]
        }
      ],
      "source": [
        "# zarebin\n",
        "\n",
        "zarebin_wrongs = read_dataset('zarebin', 'wrongs')\n",
        "zarebin_corrects = read_dataset('zarebin', 'corrects')\n",
        "zarebin_google = read_dataset('zarebin', 'google')\n",
        "zarebin_virastman = read_dataset('zarebin', 'virastman')\n",
        "zarebin_paknevis = read_dataset('zarebin', 'paknevis')\n",
        "zarebin_behnevis = read_dataset('zarebin', 'behnevis')\n",
        "\n",
        "print(f'lengths: {len(zarebin_virastman)}, {len(zarebin_paknevis)}, {len(zarebin_behnevis)}, {len(zarebin_google)}' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dl7RlsnzeD",
        "outputId": "8272d0da-bd8b-4191-e803-e1595a2b1b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lengths: 1127, 1127, 1127\n"
          ]
        }
      ],
      "source": [
        "# PerSpellData\n",
        "\n",
        "PerSpellData_wrongs = read_dataset('PerSpellData', 'wrongs')\n",
        "PerSpellData_corrects = read_dataset('PerSpellData', 'corrects')\n",
        "PerSpellData_virastman = read_dataset('PerSpellData', 'virastman')\n",
        "PerSpellData_paknevis = read_dataset('PerSpellData', 'paknevis')\n",
        "PerSpellData_behnevis = read_dataset('PerSpellData', 'behnevis')\n",
        "\n",
        "print(f'lengths: {len(PerSpellData_virastman)}, {len(PerSpellData_paknevis)}, {len(PerSpellData_behnevis)}' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRGNlTXCoqzg",
        "outputId": "0eeee512-0983-43c9-9c8f-69b36e301663"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(223, 223, 223, 223, 223)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# shargh\n",
        "\n",
        "shargh_wrongs = read_dataset('shargh', 'wrongs')\n",
        "shargh_corrects = read_dataset('shargh', 'corrects')\n",
        "shargh_virastman = read_dataset('shargh', 'virastman')\n",
        "shargh_paknevis = read_dataset('shargh', 'paknevis')\n",
        "shargh_behnevis = read_dataset('shargh', 'behnevis')\n",
        "\n",
        "len(shargh_corrects), len(shargh_wrongs), len(shargh_virastman), len(shargh_paknevis), len(shargh_behnevis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkQDhcAOoWeM",
        "outputId": "27c8b407-7adc-462d-e0f3-539d15b0072f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lengths: 451, 451\n"
          ]
        }
      ],
      "source": [
        "# dadma news 451\n",
        "\n",
        "dadma_wrongs = read_dataset('dadma-news-451', 'wrongs')\n",
        "dadma_corrects = read_dataset('dadma-news-451', 'corrects')\n",
        "dadma_paknevis = read_dataset('dadma-news-451', 'paknevis')\n",
        "dadma_behnevis = read_dataset('dadma-news-451', 'behnevis')\n",
        "\n",
        "print(f'lengths: {len(dadma_paknevis)}, {len(dadma_behnevis)}' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e78Hxvnrovba",
        "outputId": "ae5f0025-62fd-4d72-ac22-e44f9cd70169"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "539"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# dadma news title 539\n",
        "\n",
        "dadma_titles_wrongs = read_dataset('dadma-news-title-539', 'wrongs')\n",
        "dadma_titles_corrects = read_dataset('dadma-news-title-539', 'corrects')\n",
        "dadma_titles_google = read_dataset('dadma-news-title-539', 'google')\n",
        "dadma_titles_virastman = read_dataset('dadma-news-title-539', 'virastman')\n",
        "dadma_titles_paknevis = read_dataset('dadma-news-title-539', 'paknevis')\n",
        "dadma_titles_behnevis = read_dataset('dadma-news-title-539', 'behnevis')\n",
        "\n",
        "len(dadma_titles_wrongs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpLD7Lk5oyel",
        "outputId": "4734604e-59b4-43c6-fd85-06f8431de369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lengths: 19421, 19421, 19421\n"
          ]
        }
      ],
      "source": [
        "# dadma news title all\n",
        "\n",
        "dadma_titles_all_wrongs = read_dataset('dadma-news-title-all', 'wrongs')\n",
        "dadma_titles_all_corrects = read_dataset('dadma-news-title-all', 'corrects')\n",
        "dadma_titles_all_paknevis = read_dataset('dadma-news-title-all', 'paknevis')\n",
        "dadma_titles_all_behnevis = read_dataset('dadma-news-title-all', 'behnevis')\n",
        "\n",
        "print(f'lengths: {len(dadma_titles_all_paknevis)}, {len(dadma_titles_all_behnevis)}, {len(dadma_titles_all_corrects)}' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfdlxc_kTRSL"
      },
      "source": [
        "## توابع ارزیابی"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxYtOm53TTYS"
      },
      "outputs": [],
      "source": [
        "from nltk import edit_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxrNO_aGTTYT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_tok_info(index, w_tok, c_tokens, p_tokens, p_tokens_without_special, p_tokens_subwords, p_tokens_bi, verbose=False):\n",
        "  string_file = ''\n",
        "  output_info = {\n",
        "      'wrong': False,\n",
        "      'not_detected': False,\n",
        "      'detected': False,\n",
        "      'c2w': False,\n",
        "      'w2c': False,\n",
        "      'c2c': False,\n",
        "  }\n",
        "  if verbose:\n",
        "    print('\\n', w_tok)\n",
        "  is_correct = pseudo_is_contain(query=c_tokens, key=w_tok, key_index=index)\n",
        "  is_changed = not pseudo_is_contain(\n",
        "      query=p_tokens, key=w_tok, key_index=index\n",
        "      ) and not pseudo_is_contain(\n",
        "          query=p_tokens_without_special, key=w_tok, key_index=index\n",
        "      ) and not pseudo_is_contain(\n",
        "          query=p_tokens_subwords, key=w_tok, key_index=index\n",
        "      ) and not pseudo_is_contain(\n",
        "          query=p_tokens_bi, key=w_tok, key_index=index\n",
        "      ) \n",
        "\n",
        "  if not is_correct:\n",
        "    output_info['wrong'] = True\n",
        "    if verbose:\n",
        "      print('wrong', w_tok)\n",
        "    # string_file +=  f'wrong {w_tok}\\n'\n",
        "\n",
        "  if not is_correct and not is_changed:\n",
        "    output_info['not_detected'] = True\n",
        "    # string_file += f'not_detected {w_tok}\\n'\n",
        "    if verbose:\n",
        "      print('not_detected', w_tok)\n",
        "\n",
        "  if is_correct and is_changed:\n",
        "    output_info['c2w'] = True\n",
        "    if verbose:\n",
        "      print('c2w', w_tok)\n",
        "    string_file += f'c2w {w_tok}\\n'\n",
        "\n",
        "  if is_correct and not is_changed:\n",
        "    output_info['c2c'] = True\n",
        "    if verbose:\n",
        "      print('c2c', w_tok)\n",
        "    # string_file += f'c2c {w_tok}\\n'\n",
        "\n",
        "  if (not is_correct) and is_changed:\n",
        "    output_info['detected'] = True\n",
        "    if verbose:\n",
        "      print('detected', w_tok)\n",
        "    #w_detected += 1\n",
        "    # string_file += f'detected {w_tok}\\n'\n",
        "\n",
        "    c_paired_tok = get_paired_token(query=c_tokens, key=w_tok, key_index=index)\n",
        "    p_paired_tok = get_paired_token(query=p_tokens, key=w_tok, key_index=index)\n",
        "    p_paired_tok_without_special = get_paired_token(query=p_tokens_without_special, key=w_tok, key_index=index)\n",
        "    p_paired_tok_subwords = get_paired_token(query=p_tokens_subwords, key=w_tok, key_index=index)\n",
        "    p_paired_tok_bi = get_paired_token(query=p_tokens_bi, key=w_tok, key_index=index)\n",
        "    if verbose:\n",
        "      print(c_paired_tok, p_paired_tok, p_paired_tok_without_special, p_paired_tok_subwords)\n",
        "    if is_soft_equal(\n",
        "        c_paired_tok, p_paired_tok\n",
        "        ) or is_soft_equal(\n",
        "            c_paired_tok, p_paired_tok_without_special\n",
        "        ) or is_soft_equal(\n",
        "            c_paired_tok, p_paired_tok_subwords\n",
        "        ) or is_soft_equal(\n",
        "            c_paired_tok, p_paired_tok_bi\n",
        "        ):    \n",
        "      output_info['w2c'] = True\n",
        "      if verbose:\n",
        "        print('w2c', w_tok)\n",
        "      # string_file += f'w2c {w_tok}\\n'\n",
        "      \n",
        "  return output_info, string_file\n",
        "\n",
        "\n",
        "def get_paired_token(query, key, key_index, r=4):\n",
        "  most_similar = ''\n",
        "  most_similar_score = len(query)\n",
        "  for query_token in query[max(0, key_index - r): min(len(query), key_index + r)]:\n",
        "    distance = edit_distance(query_token, key)\n",
        "    if most_similar_score > distance:\n",
        "      most_similar = query_token\n",
        "      most_similar_score = distance\n",
        "  return most_similar\n",
        "\n",
        "def is_soft_equal(c_token, p_token):\n",
        "  return c_token == p_token or c_token == remove_special_chars(p_token)\n",
        "\n",
        "\n",
        "def remove_special_chars(text):\n",
        "  for special_ch in special_chars:\n",
        "    text = text.replace(special_ch, '')\n",
        "  return text\n",
        "\n",
        "def replace_halfspaces(text, replace_with=' '):\n",
        "  text = text.replace('\\\\u200c', replace_with)\n",
        "  return text.replace('\\u200c', replace_with)\n",
        "\n",
        "\n",
        "def pseudo_is_contain(query, key, key_index, r=4):\n",
        "  return key in query[max(0, key_index - r): min(len(query), key_index + r)]\n",
        "\n",
        "\n",
        "def words_halfspaces (list_of_words):\n",
        "  list_of_words = [word.split('‌') for word in list_of_words]\n",
        "  return [sub for words in list_of_words for sub in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjQYd5kTTYW"
      },
      "outputs": [],
      "source": [
        "def update_eval_params(eval_params, info):\n",
        "  for i in info:\n",
        "    eval_params[i] += info[i]\n",
        "  return eval_params\n",
        "\n",
        "def best_info (info1, info2):\n",
        "  if info2['c2c'] or info2['w2c']:\n",
        "    return info2, 2\n",
        "  return info1, 1\n",
        "\n",
        "def eval(wrong_sentence, correct_sentence, predicted_sentence, verbose=False):\n",
        "    eval_params = {\n",
        "          'wrong': 0,\n",
        "          'not_detected': 0,\n",
        "          'detected': 0,\n",
        "          'c2w': 0,\n",
        "          'w2c': 0,\n",
        "          'c2c': 0,    \n",
        "    }\n",
        "    string_file_all = ''\n",
        "    w_tokens = char_refinement(wrong_sentence).split()\n",
        "    c_tokens = char_refinement(correct_sentence).split()\n",
        "    p_tokens = char_refinement(predicted_sentence).split()\n",
        "    \n",
        "    w_tokens_bi = [f'{w1}{w2}' for w1, w2 in zip(w_tokens, w_tokens[1:])]\n",
        "    c_tokens_bi = [f'{c1}{c2}' for c1, c2 in zip(c_tokens, c_tokens[1:])]\n",
        "\n",
        "    p_tokens = [p.replace('\\\\u200c', '\\u200c') for p in p_tokens]\n",
        "    p_tokens_special_removed = [remove_special_chars(p) for p in p_tokens]\n",
        "    p_tokens_subwords = words_halfspaces(p_tokens)\n",
        "    p_tokens_bi = [f'{p1}{p2}' for p1, p2 in zip(p_tokens, p_tokens[1:])]\n",
        "    c_tokens = [remove_special_chars(c) for c in c_tokens]\n",
        "    w_tokens = [remove_special_chars(w) for w in w_tokens]\n",
        "    if verbose:\n",
        "      print('in', w_tokens)\n",
        "      print('correct', c_tokens)\n",
        "      print('origin', p_tokens)\n",
        "      print('removed', p_tokens_special_removed)\n",
        "      print('sub', p_tokens_subwords)\n",
        "      print('bi', p_tokens_bi)\n",
        "    double_continue_flag = False\n",
        "    for index, w_t in enumerate(w_tokens):\n",
        "      if double_continue_flag:\n",
        "        double_continue_flag = False\n",
        "        continue\n",
        "      which = 1\n",
        "      info, string_file = get_tok_info(index, w_t, c_tokens, p_tokens, p_tokens_special_removed, p_tokens_subwords, p_tokens_bi)\n",
        "\n",
        "      if index < len(w_tokens)-1:\n",
        "        w_tok = w_tokens_bi[index]\n",
        "\n",
        "        info_bi, string_file_bi = get_tok_info(index, w_tok, c_tokens_bi, p_tokens, p_tokens_special_removed, p_tokens_subwords, len(p_tokens_bi)*[' '])\n",
        "        \n",
        "        info, which = best_info(info, info_bi)\n",
        "        if which == 2:\n",
        "          if string_file_bi != '' :\n",
        "            string_file_all += '** bi\\n **' + string_file_bi\n",
        "          double_continue_flag = True\n",
        "\n",
        "\n",
        "      eval_params = update_eval_params(eval_params, info)\n",
        "      if which == 1 :\n",
        "        if string_file != '' :\n",
        "          string_file_all += '** normal\\n **' + string_file\n",
        "    # string_file_all += f'{eval_params}'\n",
        "    # print(eval_params)\n",
        "    return eval_params, string_file_all\n",
        "\n",
        "def write_log(string_to_file, names):\n",
        "  with open(f'{names[0]}_{names[1]}_log.txt', 'w') as f:\n",
        "    f.write(string_to_file)\n",
        "\n",
        "\n",
        "def eval_spell_checker (wrongs, corrects, predicts, names):\n",
        "  eval_params = {\n",
        "          'wrong': 0,\n",
        "          'not_detected': 0,\n",
        "          'detected': 0,\n",
        "          'c2w': 0,\n",
        "          'w2c': 0,\n",
        "          'c2c': 0,    \n",
        "  }\n",
        "  string_file = ''\n",
        "  for index, (wrong_sentence, correct_sentence, predicted_sentence) in enumerate(zip(wrongs, corrects, predicts)):\n",
        "    sentence_eval_params, string_file_sentence = eval(wrong_sentence, correct_sentence, predicted_sentence)\n",
        "    if string_file_sentence != '':\n",
        "      string_file += f'\\n{index}\\n'\n",
        "      string_file += wrong_sentence +'\\n'\n",
        "      string_file += correct_sentence +'\\n'\n",
        "      string_file += predicted_sentence + '\\n'\n",
        "    \n",
        "    eval_params = update_eval_params(eval_params, sentence_eval_params)\n",
        "    string_file += string_file_sentence\n",
        "  # print(eval_params)\n",
        "  write_log(string_file, names)\n",
        "  print()\n",
        "  w2w = eval_params['detected'] - eval_params['w2c']\n",
        "  wd_rate = round(eval_params['detected']/eval_params['wrong'], 4)\n",
        "  w2c_rate = round(eval_params['w2c']/eval_params['wrong'], 4)\n",
        "  c2w_rate = round(eval_params['c2w']/(eval_params['c2w'] + eval_params['c2c']), 4)\n",
        "  precision = round((eval_params['w2c'] + w2w)/(eval_params['c2w'] + eval_params['w2c'] + w2w) , 4)\n",
        "  print(f'{names[1]} on {names[0]}')\n",
        "  print(f'wrong detection rate:{wd_rate} \\twrong correction rate:{w2c_rate} \\tcorrect to wrong rate:{c2w_rate} \\tprecision:{precision}')\n",
        "  print(eval_params)\n",
        "  return eval_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvV4dGb8GD19",
        "outputId": "9b3a167f-6e75-406b-dd93-4a219d6a98f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Behnevis on Dadma news titles\n",
            "wrong detection rate:1.0 \twrong correction rate:0.7778 \tcorrect to wrong rate:0.0125 \tprecision:0.9\n",
            "{'wrong': 9, 'not_detected': 0, 'detected': 9, 'c2w': 1, 'w2c': 7, 'c2c': 79}\n"
          ]
        }
      ],
      "source": [
        "eval_dadma_titles_behnevis = eval_spell_checker (dadma_titles_wrongs[:10], dadma_titles_corrects[:10], dadma_titles_behnevis[:10], ['Dadma news titles', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d5S_6BDzmVSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kifmkDDamWvT"
      },
      "source": [
        "## اجرای ارزیابی"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197bf020-0ecc-4586-d742-92643469747b",
        "id": "aXcAjL90mWvV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google on Zarebin\n",
            "wrong detection rate:0.9318 \twrong correction rate:0.914 \tcorrect to wrong rate:0.0026 \tprecision:0.9962\n",
            "{'wrong': 1407, 'not_detected': 96, 'detected': 1311, 'c2w': 5, 'w2c': 1286, 'c2c': 1887}\n",
            "\n",
            "Virastman on Zarebin\n",
            "wrong detection rate:0.8342 \twrong correction rate:0.8028 \tcorrect to wrong rate:0.0006 \tprecision:0.9991\n",
            "{'wrong': 1405, 'not_detected': 233, 'detected': 1172, 'c2w': 1, 'w2c': 1128, 'c2c': 1780}\n",
            "\n",
            "Paknevis on Zarebin\n",
            "wrong detection rate:0.877 \twrong correction rate:0.7889 \tcorrect to wrong rate:0.0317 \tprecision:0.9558\n",
            "{'wrong': 1407, 'not_detected': 173, 'detected': 1234, 'c2w': 57, 'w2c': 1110, 'c2c': 1739}\n",
            "\n",
            "Behnevis on Zarebin\n",
            "wrong detection rate:0.8955 \twrong correction rate:0.8131 \tcorrect to wrong rate:0.0174 \tprecision:0.9752\n",
            "{'wrong': 1407, 'not_detected': 147, 'detected': 1260, 'c2w': 32, 'w2c': 1144, 'c2c': 1804}\n"
          ]
        }
      ],
      "source": [
        "# Result on Zarebin\n",
        "eval_zarebin_google = eval_spell_checker (zarebin_wrongs, zarebin_corrects, zarebin_google, ['Zarebin', 'Google'])\n",
        "eval_zarebin_virastman = eval_spell_checker (zarebin_wrongs, zarebin_corrects, zarebin_virastman, ['Zarebin', 'Virastman'])\n",
        "eval_zarebin_paknevis = eval_spell_checker (zarebin_wrongs, zarebin_corrects, zarebin_paknevis, ['Zarebin', 'Paknevis'])\n",
        "eval_zarebin_behevis = eval_spell_checker (zarebin_wrongs, zarebin_corrects, zarebin_behnevis, ['Zarebin', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babb29d2-bba4-4137-ef22-2038e6dfd608",
        "id": "62PiRKsXmWva"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Virastman on PerSpellData\n",
            "wrong detection rate:0.7952 \twrong correction rate:0.6508 \tcorrect to wrong rate:0.0008 \tprecision:0.9892\n",
            "{'wrong': 1157, 'not_detected': 237, 'detected': 920, 'c2w': 10, 'w2c': 753, 'c2c': 13252}\n",
            "\n",
            "Paknevis on PerSpellData\n",
            "wrong detection rate:0.9455 \twrong correction rate:0.8245 \tcorrect to wrong rate:0.016 \tprecision:0.8396\n",
            "{'wrong': 1157, 'not_detected': 63, 'detected': 1094, 'c2w': 209, 'w2c': 954, 'c2c': 12848}\n",
            "\n",
            "Behnevis on PerSpellData\n",
            "wrong detection rate:0.9196 \twrong correction rate:0.8358 \tcorrect to wrong rate:0.0034 \tprecision:0.9594\n",
            "{'wrong': 1157, 'not_detected': 93, 'detected': 1064, 'c2w': 45, 'w2c': 967, 'c2c': 13321}\n"
          ]
        }
      ],
      "source": [
        "# Result on PerSpellData\n",
        "\n",
        "eval_perspell_virastman = eval_spell_checker (PerSpellData_wrongs, PerSpellData_corrects, PerSpellData_virastman, ['PerSpellData', 'Virastman'])\n",
        "eval_perspell_paknevis = eval_spell_checker (PerSpellData_wrongs, PerSpellData_corrects, PerSpellData_paknevis, ['PerSpellData', 'Paknevis'])\n",
        "eval_perspell_behnevis = eval_spell_checker (PerSpellData_wrongs, PerSpellData_corrects, PerSpellData_behnevis, ['PerSpellData', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd8f98c-b3ef-4bdc-f730-004bebf9eb1e",
        "id": "2lABT0tAmWv6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Virastman on Shargh\n",
            "wrong detection rate:0.6036 \twrong correction rate:0.482 \tcorrect to wrong rate:0.0 \tprecision:1.0\n",
            "{'wrong': 222, 'not_detected': 88, 'detected': 134, 'c2w': 0, 'w2c': 107, 'c2c': 1575}\n",
            "\n",
            "Paknevis on Shargh\n",
            "wrong detection rate:0.7568 \twrong correction rate:0.5811 \tcorrect to wrong rate:0.0295 \tprecision:0.785\n",
            "{'wrong': 222, 'not_detected': 54, 'detected': 168, 'c2w': 46, 'w2c': 129, 'c2c': 1515}\n",
            "\n",
            "Behnevis on Shargh\n",
            "wrong detection rate:0.8243 \twrong correction rate:0.6486 \tcorrect to wrong rate:0.0038 \tprecision:0.9683\n",
            "{'wrong': 222, 'not_detected': 39, 'detected': 183, 'c2w': 6, 'w2c': 144, 'c2c': 1579}\n"
          ]
        }
      ],
      "source": [
        "# Result on Shargh\n",
        "\n",
        "eval_shargh_virastman = eval_spell_checker (shargh_wrongs, shargh_corrects, shargh_virastman, ['Shargh', 'Virastman'])\n",
        "eval_shargh_paknevis = eval_spell_checker (shargh_wrongs, shargh_corrects, shargh_paknevis, ['Shargh', 'Paknevis'])\n",
        "eval_shargh_behnevis = eval_spell_checker (shargh_wrongs, shargh_corrects, shargh_behnevis, ['Shargh', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9f7a70-4cad-4b05-eb83-d601461d8b30",
        "id": "MkZK32fFmWv9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paknevis on Dadma\n",
            "wrong detection rate:0.7931 \twrong correction rate:0.6607 \tcorrect to wrong rate:0.0216 \tprecision:0.9067\n",
            "{'wrong': 2305, 'not_detected': 477, 'detected': 1828, 'c2w': 188, 'w2c': 1523, 'c2c': 8510}\n",
            "\n",
            "Behnevis on Dadma\n",
            "wrong detection rate:0.8386 \twrong correction rate:0.7367 \tcorrect to wrong rate:0.0037 \tprecision:0.9832\n",
            "{'wrong': 2305, 'not_detected': 372, 'detected': 1933, 'c2w': 33, 'w2c': 1698, 'c2c': 8827}\n"
          ]
        }
      ],
      "source": [
        "# Result on Dadma \n",
        "\n",
        "eval_dadma_paknevis = eval_spell_checker (dadma_wrongs, dadma_corrects, dadma_paknevis, ['Dadma', 'Paknevis'])\n",
        "eval_dadma_behnevis = eval_spell_checker (dadma_wrongs, dadma_corrects, dadma_behnevis, ['Dadma', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2dceeba-8913-4846-b0ad-fdeaa825a9cc",
        "id": "3xxe2GmkmWv_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google on Dadma news titles\n",
            "wrong detection rate:0.7392 \twrong correction rate:0.702 \tcorrect to wrong rate:0.0045 \tprecision:0.9449\n",
            "{'wrong': 510, 'not_detected': 133, 'detected': 377, 'c2w': 22, 'w2c': 358, 'c2c': 4848}\n",
            "\n",
            "Virastman on Dadma news titles\n",
            "wrong detection rate:0.6 \twrong correction rate:0.4863 \tcorrect to wrong rate:0.0034 \tprecision:0.9503\n",
            "{'wrong': 510, 'not_detected': 204, 'detected': 306, 'c2w': 16, 'w2c': 248, 'c2c': 4685}\n",
            "\n",
            "Paknevis on Dadma news titles\n",
            "wrong detection rate:0.7843 \twrong correction rate:0.6706 \tcorrect to wrong rate:0.0228 \tprecision:0.7921\n",
            "{'wrong': 510, 'not_detected': 110, 'detected': 400, 'c2w': 105, 'w2c': 342, 'c2c': 4497}\n",
            "\n",
            "Behnevis on Dadma news titles\n",
            "wrong detection rate:0.8314 \twrong correction rate:0.7216 \tcorrect to wrong rate:0.003 \tprecision:0.968\n",
            "{'wrong': 510, 'not_detected': 86, 'detected': 424, 'c2w': 14, 'w2c': 368, 'c2c': 4691}\n"
          ]
        }
      ],
      "source": [
        "# Result on dadma news title 539\n",
        "\n",
        "eval_dadma_titles_google = eval_spell_checker (dadma_titles_wrongs, dadma_titles_corrects, dadma_titles_google, ['Dadma news titles', 'Google'])\n",
        "eval_dadma_titles_virastman = eval_spell_checker (dadma_titles_wrongs, dadma_titles_corrects, dadma_titles_virastman, ['Dadma news titles', 'Virastman'])\n",
        "eval_dadma_titles_paknevis = eval_spell_checker (dadma_titles_wrongs, dadma_titles_corrects, dadma_titles_paknevis, ['Dadma news titles', 'Paknevis'])\n",
        "eval_dadma_titles_behnevis = eval_spell_checker (dadma_titles_wrongs, dadma_titles_corrects, dadma_titles_behnevis, ['Dadma news titles', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47ace23-0d8b-4b93-bd76-e44fedbc5fa7",
        "id": "IFeoaeb5mWwB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paknevis on Dadma\n",
            "wrong detection rate:0.7794 \twrong correction rate:0.6451 \tcorrect to wrong rate:0.0233 \tprecision:0.7785\n",
            "{'wrong': 18365, 'not_detected': 4051, 'detected': 14314, 'c2w': 4072, 'w2c': 11847, 'c2c': 170600}\n",
            "\n",
            "Behnevis on Dadma\n",
            "wrong detection rate:0.827 \twrong correction rate:0.7095 \tcorrect to wrong rate:0.0085 \tprecision:0.9097\n",
            "{'wrong': 18368, 'not_detected': 3177, 'detected': 15191, 'c2w': 1507, 'w2c': 13032, 'c2c': 176132}\n"
          ]
        }
      ],
      "source": [
        "# Result on dadma news title\n",
        "\n",
        "eval_dadma_titles_all_paknevis = eval_spell_checker (dadma_titles_all_wrongs, dadma_titles_all_corrects, dadma_titles_all_paknevis, ['Dadma', 'Paknevis'])\n",
        "eval_dadma_titles_all_behnevis = eval_spell_checker (dadma_titles_all_wrongs, dadma_titles_all_corrects, dadma_titles_all_behnevis, ['Dadma', 'Behnevis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAi0terVmWwD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Persian Spell checkers comparison.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}